//
// Generated by LLVM NVPTX Back-End
//

.version 3.2
.target sm_30
.address_size 64

	// .globl	printThreadIndex
.extern .func  (.param .b32 func_retval0) vprintf
(
	.param .b64 vprintf_param_0,
	.param .b64 vprintf_param_1
)
;
.global .align 1 .b8 anon_$_acdf3ec124034ec8299c96255fecd0d7_$_0[100] = {116, 104, 114, 101, 97, 100, 95, 105, 100, 32, 40, 37, 117, 108, 44, 32, 37, 117, 108, 41, 32, 32, 32, 98, 108, 111, 99, 107, 95, 105, 100, 32, 40, 37, 117, 108, 44, 32, 37, 117, 108, 41, 32, 32, 32, 99, 111, 111, 114, 100, 105, 110, 97, 116, 101, 32, 40, 37, 117, 108, 44, 32, 37, 117, 108, 41, 32, 32, 32, 32, 103, 108, 111, 98, 97, 108, 95, 105, 100, 120, 32, 37, 117, 108, 44, 32, 32, 32, 32, 105, 118, 97, 108, 58, 32, 32, 32, 37, 102, 10};

.visible .entry printThreadIndex(
	.param .u64 printThreadIndex_param_0,
	.param .u64 printThreadIndex_param_1,
	.param .u64 printThreadIndex_param_2,
	.param .u32 printThreadIndex_param_3,
	.param .u32 printThreadIndex_param_4
)
{
	.local .align 8 .b8 	__local_depot0[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<9>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<22>;

	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [printThreadIndex_param_0];
	cvta.to.global.u64 	%rd2, %rd1;
	ld.param.u64 	%rd3, [printThreadIndex_param_1];
	add.u64 	%rd4, %SP, 0;
	add.u64 	%rd5, %SPL, 0;
	ld.param.u32 	%rd6, [printThreadIndex_param_3];
	ld.param.u32 	%rd7, [printThreadIndex_param_4];
	mov.u32 	%r1, %tid.x;
	cvt.s64.s32 	%rd8, %r1;
	mov.u32 	%r2, %ctaid.x;
	cvt.s64.s32 	%rd9, %r2;
	mul.lo.s64 	%rd10, %rd9, %rd6;
	add.s64 	%rd11, %rd10, %rd8;
	mov.u32 	%r3, %tid.y;
	cvt.s64.s32 	%rd12, %r3;
	mov.u32 	%r4, %ctaid.y;
	cvt.s64.s32 	%rd13, %r4;
	mul.lo.s64 	%rd14, %rd13, %rd7;
	add.s64 	%rd15, %rd14, %rd12;
	mul.lo.s64 	%rd16, %rd15, %rd3;
	add.s64 	%rd17, %rd16, %rd11;
	cvt.u32.u64 	%r5, %rd11;
	cvt.u32.u64 	%r6, %rd15;
	shl.b64 	%rd18, %rd17, 2;
	add.s64 	%rd19, %rd2, %rd18;
	ld.global.f32 	%f1, [%rd19];
	cvt.f64.f32 	%fd1, %f1;
	st.local.v2.u32 	[%rd5], {%r1, %r3};
	st.local.v2.u32 	[%rd5+8], {%r2, %r4};
	st.local.v2.u32 	[%rd5+16], {%r5, %r6};
	st.local.u32 	[%rd5+24], %rd17;
	st.local.f64 	[%rd5+32], %fd1;
	mov.u64 	%rd20, anon_$_acdf3ec124034ec8299c96255fecd0d7_$_0;
	cvta.global.u64 	%rd21, %rd20;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd21;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd4;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r7, [retval0+0];
	} // callseq 0
	ret;

}
	// .globl	sumMatrixOnGpu2D2D
.visible .entry sumMatrixOnGpu2D2D(
	.param .u64 sumMatrixOnGpu2D2D_param_0,
	.param .u64 sumMatrixOnGpu2D2D_param_1,
	.param .u64 sumMatrixOnGpu2D2D_param_2,
	.param .u64 sumMatrixOnGpu2D2D_param_3,
	.param .u64 sumMatrixOnGpu2D2D_param_4,
	.param .u32 sumMatrixOnGpu2D2D_param_5,
	.param .u32 sumMatrixOnGpu2D2D_param_6
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<25>;

	ld.param.u64 	%rd10, [sumMatrixOnGpu2D2D_param_3];
	ld.param.u64 	%rd11, [sumMatrixOnGpu2D2D_param_4];
	mov.u32 	%r1, %tid.x;
	cvt.s64.s32 	%rd12, %r1;
	ld.param.u32 	%rd13, [sumMatrixOnGpu2D2D_param_5];
	ld.param.u32 	%rd14, [sumMatrixOnGpu2D2D_param_6];
	mov.u32 	%r2, %ctaid.x;
	cvt.s64.s32 	%rd15, %r2;
	mul.lo.s64 	%rd16, %rd15, %rd13;
	add.s64 	%rd17, %rd16, %rd12;
	mov.u32 	%r3, %tid.y;
	cvt.s64.s32 	%rd18, %r3;
	mov.u32 	%r4, %ctaid.y;
	cvt.s64.s32 	%rd19, %r4;
	mul.lo.s64 	%rd20, %rd19, %rd14;
	add.s64 	%rd21, %rd20, %rd18;
	setp.ge.s64 	%p1, %rd17, %rd10;
	setp.ge.s64 	%p2, %rd21, %rd11;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	LBB1_2;
	ld.param.u64 	%rd4, [sumMatrixOnGpu2D2D_param_0];
	ld.param.u64 	%rd5, [sumMatrixOnGpu2D2D_param_2];
	cvta.to.global.u64 	%rd6, %rd5;
	ld.param.u64 	%rd7, [sumMatrixOnGpu2D2D_param_1];
	cvta.to.global.u64 	%rd8, %rd7;
	cvta.to.global.u64 	%rd9, %rd4;
	mul.lo.s64 	%rd22, %rd21, %rd10;
	add.s64 	%rd23, %rd22, %rd17;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd1, %rd9, %rd24;
	add.s64 	%rd2, %rd8, %rd24;
	add.s64 	%rd3, %rd6, %rd24;
	ld.global.f32 	%f1, [%rd1];
	ld.global.f32 	%f2, [%rd2];
	add.rn.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd3], %f3;
LBB1_2:
	ret;

}
	// .globl	sumMatrixOnGpu2D1D
.visible .entry sumMatrixOnGpu2D1D(
	.param .u64 sumMatrixOnGpu2D1D_param_0,
	.param .u64 sumMatrixOnGpu2D1D_param_1,
	.param .u64 sumMatrixOnGpu2D1D_param_2,
	.param .u64 sumMatrixOnGpu2D1D_param_3,
	.param .u64 sumMatrixOnGpu2D1D_param_4,
	.param .u32 sumMatrixOnGpu2D1D_param_5,
	.param .u32 sumMatrixOnGpu2D1D_param_6
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<21>;

	ld.param.u64 	%rd10, [sumMatrixOnGpu2D1D_param_3];
	ld.param.u64 	%rd11, [sumMatrixOnGpu2D1D_param_4];
	mov.u32 	%r1, %tid.x;
	cvt.s64.s32 	%rd12, %r1;
	ld.param.u32 	%rd13, [sumMatrixOnGpu2D1D_param_5];
	mov.u32 	%r2, %ctaid.x;
	cvt.s64.s32 	%rd14, %r2;
	mul.lo.s64 	%rd15, %rd14, %rd13;
	add.s64 	%rd16, %rd15, %rd12;
	mov.u32 	%r3, %ctaid.y;
	cvt.s64.s32 	%rd17, %r3;
	setp.ge.s64 	%p1, %rd16, %rd10;
	setp.ge.s64 	%p2, %rd17, %rd11;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	LBB2_2;
	ld.param.u64 	%rd4, [sumMatrixOnGpu2D1D_param_0];
	ld.param.u64 	%rd5, [sumMatrixOnGpu2D1D_param_2];
	cvta.to.global.u64 	%rd6, %rd5;
	ld.param.u64 	%rd7, [sumMatrixOnGpu2D1D_param_1];
	cvta.to.global.u64 	%rd8, %rd7;
	cvta.to.global.u64 	%rd9, %rd4;
	mul.lo.s64 	%rd18, %rd17, %rd10;
	add.s64 	%rd19, %rd18, %rd16;
	shl.b64 	%rd20, %rd19, 2;
	add.s64 	%rd1, %rd9, %rd20;
	add.s64 	%rd2, %rd8, %rd20;
	add.s64 	%rd3, %rd6, %rd20;
	ld.global.f32 	%f1, [%rd1];
	ld.global.f32 	%f2, [%rd2];
	add.rn.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd3], %f3;
LBB2_2:
	ret;

}
	// .globl	matrix_mul_2D2D
.visible .entry matrix_mul_2D2D(
	.param .u64 matrix_mul_2D2D_param_0,
	.param .u64 matrix_mul_2D2D_param_1,
	.param .u64 matrix_mul_2D2D_param_2,
	.param .u64 matrix_mul_2D2D_param_3,
	.param .u64 matrix_mul_2D2D_param_4,
	.param .u64 matrix_mul_2D2D_param_5,
	.param .u64 matrix_mul_2D2D_param_6,
	.param .u32 matrix_mul_2D2D_param_7,
	.param .u32 matrix_mul_2D2D_param_8
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<35>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<73>;

	ld.param.u64 	%rd35, [matrix_mul_2D2D_param_4];
	ld.param.u64 	%rd40, [matrix_mul_2D2D_param_5];
	setp.ne.s64 	%p1, %rd35, %rd40;
	@%p1 bra 	LBB3_2;
	ld.param.u32 	%r2, [matrix_mul_2D2D_param_8];
	ld.param.u32 	%r1, [matrix_mul_2D2D_param_7];
	ld.param.u64 	%rd36, [matrix_mul_2D2D_param_6];
	ld.param.u64 	%rd34, [matrix_mul_2D2D_param_3];
	mov.u32 	%r3, %tid.x;
	cvt.s64.s32 	%rd4, %r3;
	mov.u32 	%r4, %ctaid.x;
	cvt.s64.s32 	%rd41, %r4;
	cvt.u64.u32 	%rd42, %r1;
	mul.lo.s64 	%rd5, %rd41, %rd42;
	add.s64 	%rd6, %rd5, %rd4;
	mov.u32 	%r5, %tid.y;
	cvt.s64.s32 	%rd43, %r5;
	mov.u32 	%r6, %ctaid.y;
	cvt.s64.s32 	%rd44, %r6;
	cvt.u64.u32 	%rd45, %r2;
	mul.lo.s64 	%rd46, %rd44, %rd45;
	add.s64 	%rd7, %rd46, %rd43;
	setp.lt.s64 	%p2, %rd6, %rd36;
	setp.lt.s64 	%p3, %rd7, %rd34;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	LBB3_3;
	bra.uni 	LBB3_2;
LBB3_3:
	ld.param.u64 	%rd38, [matrix_mul_2D2D_param_2];
	cvta.to.global.u64 	%rd1, %rd38;
	mul.lo.s64 	%rd47, %rd7, %rd36;
	add.s64 	%rd8, %rd47, %rd6;
	setp.eq.s64 	%p5, %rd35, 0;
	mov.f32 	%f34, 0f00000000;
	@%p5 bra 	LBB3_10;
	ld.param.u64 	%rd37, [matrix_mul_2D2D_param_0];
	ld.param.u64 	%rd39, [matrix_mul_2D2D_param_1];
	cvta.to.global.u64 	%rd2, %rd39;
	cvta.to.global.u64 	%rd3, %rd37;
	mul.lo.s64 	%rd9, %rd7, %rd35;
	add.s64 	%rd49, %rd35, -1;
	and.b64  	%rd72, %rd35, 3;
	setp.lt.u64 	%p6, %rd49, 3;
	mov.u64 	%rd69, 0;
	mov.f32 	%f34, 0f00000000;
	shl.b64 	%rd65, %rd36, 2;
	@%p6 bra 	LBB3_7;
	sub.s64 	%rd69, %rd35, %rd72;
	shl.b64 	%rd51, %rd6, 2;
	add.s64 	%rd67, %rd2, %rd51;
	shl.b64 	%rd14, %rd36, 4;
	shl.b64 	%rd15, %rd36, 3;
	mul.lo.s64 	%rd16, %rd36, 12;
	shl.b64 	%rd52, %rd9, 2;
	add.s64 	%rd53, %rd52, %rd3;
	add.s64 	%rd66, %rd53, 8;
	mov.u64 	%rd68, 0;
	mov.f32 	%f34, 0f00000000;
LBB3_6:
	ld.global.f32 	%f12, [%rd66+-8];
	ld.global.f32 	%f13, [%rd67];
	mul.rn.f32 	%f14, %f12, %f13;
	add.rn.f32 	%f15, %f34, %f14;
	ld.global.f32 	%f16, [%rd66+-4];
	add.s64 	%rd54, %rd67, %rd65;
	ld.global.f32 	%f17, [%rd54];
	mul.rn.f32 	%f18, %f16, %f17;
	add.rn.f32 	%f19, %f15, %f18;
	ld.global.f32 	%f20, [%rd66];
	add.s64 	%rd55, %rd67, %rd15;
	ld.global.f32 	%f21, [%rd55];
	mul.rn.f32 	%f22, %f20, %f21;
	add.rn.f32 	%f23, %f19, %f22;
	add.s64 	%rd68, %rd68, 4;
	ld.global.f32 	%f24, [%rd66+4];
	add.s64 	%rd56, %rd67, %rd16;
	ld.global.f32 	%f25, [%rd56];
	mul.rn.f32 	%f26, %f24, %f25;
	add.rn.f32 	%f34, %f23, %f26;
	add.s64 	%rd67, %rd67, %rd14;
	add.s64 	%rd66, %rd66, 16;
	setp.ne.s64 	%p7, %rd69, %rd68;
	@%p7 bra 	LBB3_6;
LBB3_7:
	setp.eq.s64 	%p8, %rd72, 0;
	@%p8 bra 	LBB3_10;
	mul.lo.s64 	%rd57, %rd69, %rd36;
	add.s64 	%rd58, %rd57, %rd5;
	add.s64 	%rd59, %rd58, %rd4;
	shl.b64 	%rd60, %rd59, 2;
	add.s64 	%rd71, %rd2, %rd60;
	add.s64 	%rd61, %rd69, %rd9;
	shl.b64 	%rd62, %rd61, 2;
	add.s64 	%rd70, %rd3, %rd62;
LBB3_9:
	.pragma "nounroll";
	ld.global.f32 	%f27, [%rd70];
	ld.global.f32 	%f28, [%rd71];
	mul.rn.f32 	%f29, %f27, %f28;
	add.rn.f32 	%f34, %f34, %f29;
	add.s64 	%rd72, %rd72, -1;
	add.s64 	%rd71, %rd71, %rd65;
	add.s64 	%rd70, %rd70, 4;
	setp.ne.s64 	%p9, %rd72, 0;
	@%p9 bra 	LBB3_9;
LBB3_10:
	shl.b64 	%rd63, %rd8, 2;
	add.s64 	%rd64, %rd1, %rd63;
	st.global.f32 	[%rd64], %f34;
LBB3_2:
	ret;

}
	// .globl	matrix_mul_2D1D
.visible .entry matrix_mul_2D1D(
	.param .u64 matrix_mul_2D1D_param_0,
	.param .u64 matrix_mul_2D1D_param_1,
	.param .u64 matrix_mul_2D1D_param_2,
	.param .u64 matrix_mul_2D1D_param_3,
	.param .u64 matrix_mul_2D1D_param_4,
	.param .u64 matrix_mul_2D1D_param_5,
	.param .u64 matrix_mul_2D1D_param_6,
	.param .u32 matrix_mul_2D1D_param_7,
	.param .u32 matrix_mul_2D1D_param_8
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<35>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<69>;

	ld.param.u64 	%rd35, [matrix_mul_2D1D_param_4];
	ld.param.u64 	%rd40, [matrix_mul_2D1D_param_5];
	setp.ne.s64 	%p1, %rd35, %rd40;
	@%p1 bra 	LBB4_2;
	ld.param.u32 	%r1, [matrix_mul_2D1D_param_7];
	ld.param.u64 	%rd36, [matrix_mul_2D1D_param_6];
	ld.param.u64 	%rd34, [matrix_mul_2D1D_param_3];
	mov.u32 	%r2, %tid.x;
	cvt.s64.s32 	%rd4, %r2;
	mov.u32 	%r3, %ctaid.x;
	cvt.s64.s32 	%rd41, %r3;
	cvt.u64.u32 	%rd42, %r1;
	mul.lo.s64 	%rd5, %rd41, %rd42;
	add.s64 	%rd6, %rd5, %rd4;
	mov.u32 	%r4, %ctaid.y;
	cvt.s64.s32 	%rd7, %r4;
	setp.lt.s64 	%p2, %rd6, %rd36;
	setp.lt.s64 	%p3, %rd7, %rd34;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	LBB4_3;
	bra.uni 	LBB4_2;
LBB4_3:
	ld.param.u64 	%rd38, [matrix_mul_2D1D_param_2];
	cvta.to.global.u64 	%rd1, %rd38;
	mul.lo.s64 	%rd43, %rd7, %rd36;
	add.s64 	%rd8, %rd43, %rd6;
	setp.eq.s64 	%p5, %rd35, 0;
	mov.f32 	%f34, 0f00000000;
	@%p5 bra 	LBB4_10;
	ld.param.u64 	%rd37, [matrix_mul_2D1D_param_0];
	ld.param.u64 	%rd39, [matrix_mul_2D1D_param_1];
	cvta.to.global.u64 	%rd2, %rd39;
	cvta.to.global.u64 	%rd3, %rd37;
	mul.lo.s64 	%rd9, %rd7, %rd35;
	add.s64 	%rd45, %rd35, -1;
	and.b64  	%rd68, %rd35, 3;
	setp.lt.u64 	%p6, %rd45, 3;
	mov.u64 	%rd65, 0;
	mov.f32 	%f34, 0f00000000;
	shl.b64 	%rd61, %rd36, 2;
	@%p6 bra 	LBB4_7;
	sub.s64 	%rd65, %rd35, %rd68;
	shl.b64 	%rd47, %rd6, 2;
	add.s64 	%rd63, %rd2, %rd47;
	shl.b64 	%rd14, %rd36, 4;
	shl.b64 	%rd15, %rd36, 3;
	mul.lo.s64 	%rd16, %rd36, 12;
	shl.b64 	%rd48, %rd9, 2;
	add.s64 	%rd49, %rd48, %rd3;
	add.s64 	%rd62, %rd49, 8;
	mov.u64 	%rd64, 0;
	mov.f32 	%f34, 0f00000000;
LBB4_6:
	ld.global.f32 	%f12, [%rd62+-8];
	ld.global.f32 	%f13, [%rd63];
	mul.rn.f32 	%f14, %f12, %f13;
	add.rn.f32 	%f15, %f34, %f14;
	ld.global.f32 	%f16, [%rd62+-4];
	add.s64 	%rd50, %rd63, %rd61;
	ld.global.f32 	%f17, [%rd50];
	mul.rn.f32 	%f18, %f16, %f17;
	add.rn.f32 	%f19, %f15, %f18;
	ld.global.f32 	%f20, [%rd62];
	add.s64 	%rd51, %rd63, %rd15;
	ld.global.f32 	%f21, [%rd51];
	mul.rn.f32 	%f22, %f20, %f21;
	add.rn.f32 	%f23, %f19, %f22;
	add.s64 	%rd64, %rd64, 4;
	ld.global.f32 	%f24, [%rd62+4];
	add.s64 	%rd52, %rd63, %rd16;
	ld.global.f32 	%f25, [%rd52];
	mul.rn.f32 	%f26, %f24, %f25;
	add.rn.f32 	%f34, %f23, %f26;
	add.s64 	%rd63, %rd63, %rd14;
	add.s64 	%rd62, %rd62, 16;
	setp.ne.s64 	%p7, %rd65, %rd64;
	@%p7 bra 	LBB4_6;
LBB4_7:
	setp.eq.s64 	%p8, %rd68, 0;
	@%p8 bra 	LBB4_10;
	mul.lo.s64 	%rd53, %rd65, %rd36;
	add.s64 	%rd54, %rd53, %rd5;
	add.s64 	%rd55, %rd54, %rd4;
	shl.b64 	%rd56, %rd55, 2;
	add.s64 	%rd67, %rd2, %rd56;
	add.s64 	%rd57, %rd65, %rd9;
	shl.b64 	%rd58, %rd57, 2;
	add.s64 	%rd66, %rd3, %rd58;
LBB4_9:
	.pragma "nounroll";
	ld.global.f32 	%f27, [%rd66];
	ld.global.f32 	%f28, [%rd67];
	mul.rn.f32 	%f29, %f27, %f28;
	add.rn.f32 	%f34, %f34, %f29;
	add.s64 	%rd68, %rd68, -1;
	add.s64 	%rd67, %rd67, %rd61;
	add.s64 	%rd66, %rd66, 4;
	setp.ne.s64 	%p9, %rd68, 0;
	@%p9 bra 	LBB4_9;
LBB4_10:
	shl.b64 	%rd59, %rd8, 2;
	add.s64 	%rd60, %rd1, %rd59;
	st.global.f32 	[%rd60], %f34;
LBB4_2:
	ret;

}
	// .globl	nodiag_normalize
.visible .entry nodiag_normalize(
	.param .u64 nodiag_normalize_param_0,
	.param .u64 nodiag_normalize_param_1,
	.param .u64 nodiag_normalize_param_2,
	.param .u64 nodiag_normalize_param_3,
	.param .u32 nodiag_normalize_param_4,
	.param .u32 nodiag_normalize_param_5
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<25>;

	ld.param.u64 	%rd5, [nodiag_normalize_param_2];
	mov.u32 	%r1, %tid.x;
	cvt.s64.s32 	%rd9, %r1;
	ld.param.u32 	%rd10, [nodiag_normalize_param_4];
	ld.param.u32 	%rd11, [nodiag_normalize_param_5];
	mov.u32 	%r2, %ctaid.x;
	cvt.s64.s32 	%rd12, %r2;
	mul.lo.s64 	%rd13, %rd12, %rd10;
	add.s64 	%rd3, %rd13, %rd9;
	mov.u32 	%r3, %tid.y;
	cvt.s64.s32 	%rd14, %r3;
	mov.u32 	%r4, %ctaid.y;
	cvt.s64.s32 	%rd15, %r4;
	mul.lo.s64 	%rd16, %rd15, %rd11;
	add.s64 	%rd4, %rd16, %rd14;
	setp.ge.s64 	%p1, %rd3, %rd5;
	setp.ge.s64 	%p2, %rd4, %rd5;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	LBB5_3;
	ld.param.u64 	%rd6, [nodiag_normalize_param_3];
	setp.ne.s64 	%p4, %rd3, %rd6;
	setp.eq.s64 	%p5, %rd3, %rd4;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	LBB5_3;
	ld.param.u64 	%rd7, [nodiag_normalize_param_0];
	ld.param.u64 	%rd8, [nodiag_normalize_param_1];
	cvta.to.global.u64 	%rd1, %rd8;
	cvta.to.global.u64 	%rd2, %rd7;
	mul.lo.s64 	%rd17, %rd6, %rd5;
	add.s64 	%rd18, %rd4, %rd17;
	shl.b64 	%rd19, %rd18, 2;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.f32 	%f1, [%rd20];
	add.s64 	%rd21, %rd17, %rd6;
	shl.b64 	%rd22, %rd21, 2;
	add.s64 	%rd23, %rd2, %rd22;
	ld.global.f32 	%f2, [%rd23];
	div.rn.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd20], %f3;
	add.s64 	%rd24, %rd2, %rd19;
	ld.global.f32 	%f4, [%rd24];
	ld.global.f32 	%f5, [%rd23];
	div.rn.f32 	%f6, %f4, %f5;
	st.global.f32 	[%rd24], %f6;
LBB5_3:
	ret;

}
	// .globl	diag_normalize
.visible .entry diag_normalize(
	.param .u64 diag_normalize_param_0,
	.param .u64 diag_normalize_param_1,
	.param .u64 diag_normalize_param_2,
	.param .u64 diag_normalize_param_3,
	.param .u32 diag_normalize_param_4,
	.param .u32 diag_normalize_param_5
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<22>;

	ld.param.u64 	%rd5, [diag_normalize_param_2];
	mov.u32 	%r1, %tid.x;
	cvt.s64.s32 	%rd9, %r1;
	ld.param.u32 	%rd10, [diag_normalize_param_4];
	ld.param.u32 	%rd11, [diag_normalize_param_5];
	mov.u32 	%r2, %ctaid.x;
	cvt.s64.s32 	%rd12, %r2;
	mul.lo.s64 	%rd13, %rd12, %rd10;
	add.s64 	%rd3, %rd13, %rd9;
	mov.u32 	%r3, %tid.y;
	cvt.s64.s32 	%rd14, %r3;
	mov.u32 	%r4, %ctaid.y;
	cvt.s64.s32 	%rd15, %r4;
	mul.lo.s64 	%rd16, %rd15, %rd11;
	add.s64 	%rd4, %rd16, %rd14;
	setp.ge.s64 	%p1, %rd3, %rd5;
	setp.ge.s64 	%p2, %rd4, %rd5;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	LBB6_3;
	ld.param.u64 	%rd6, [diag_normalize_param_3];
	setp.ne.s64 	%p4, %rd3, %rd4;
	setp.ne.s64 	%p5, %rd3, %rd6;
	or.pred  	%p6, %p5, %p4;
	@%p6 bra 	LBB6_3;
	ld.param.u64 	%rd7, [diag_normalize_param_0];
	ld.param.u64 	%rd8, [diag_normalize_param_1];
	cvta.to.global.u64 	%rd1, %rd8;
	cvta.to.global.u64 	%rd2, %rd7;
	mul.lo.s64 	%rd17, %rd6, %rd5;
	add.s64 	%rd18, %rd17, %rd6;
	shl.b64 	%rd19, %rd18, 2;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.f32 	%f1, [%rd20];
	add.s64 	%rd21, %rd2, %rd19;
	ld.global.f32 	%f2, [%rd21];
	div.rn.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd20], %f3;
	ld.global.f32 	%f4, [%rd21];
	div.rn.f32 	%f5, %f4, %f4;
	st.global.f32 	[%rd21], %f5;
LBB6_3:
	ret;

}
	// .globl	gaussjordan
.visible .entry gaussjordan(
	.param .u64 gaussjordan_param_0,
	.param .u64 gaussjordan_param_1,
	.param .u64 gaussjordan_param_2,
	.param .u64 gaussjordan_param_3,
	.param .u32 gaussjordan_param_4,
	.param .u32 gaussjordan_param_5
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<32>;

	ld.param.u64 	%rd9, [gaussjordan_param_3];
	ld.param.u64 	%rd8, [gaussjordan_param_2];
	mov.u32 	%r1, %tid.x;
	cvt.s64.s32 	%rd12, %r1;
	ld.param.u32 	%rd13, [gaussjordan_param_4];
	ld.param.u32 	%rd14, [gaussjordan_param_5];
	mov.u32 	%r2, %ctaid.x;
	cvt.s64.s32 	%rd15, %r2;
	mul.lo.s64 	%rd16, %rd15, %rd13;
	add.s64 	%rd3, %rd16, %rd12;
	mov.u32 	%r3, %tid.y;
	cvt.s64.s32 	%rd17, %r3;
	mov.u32 	%r4, %ctaid.y;
	cvt.s64.s32 	%rd18, %r4;
	mul.lo.s64 	%rd19, %rd18, %rd14;
	add.s64 	%rd4, %rd19, %rd17;
	setp.ge.s64 	%p1, %rd3, %rd8;
	setp.ge.s64 	%p2, %rd4, %rd8;
	or.pred  	%p3, %p1, %p2;
	setp.eq.s64 	%p4, %rd3, %rd9;
	or.pred  	%p5, %p4, %p3;
	@%p5 bra 	LBB7_3;
	ld.param.u64 	%rd10, [gaussjordan_param_0];
	ld.param.u64 	%rd11, [gaussjordan_param_1];
	cvta.to.global.u64 	%rd1, %rd11;
	cvta.to.global.u64 	%rd2, %rd10;
	mul.lo.s64 	%rd20, %rd3, %rd8;
	add.s64 	%rd5, %rd4, %rd20;
	shl.b64 	%rd21, %rd5, 2;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.f32 	%f1, [%rd22];
	mul.lo.s64 	%rd23, %rd9, %rd8;
	add.s64 	%rd6, %rd4, %rd23;
	shl.b64 	%rd24, %rd6, 2;
	add.s64 	%rd25, %rd1, %rd24;
	ld.global.f32 	%f2, [%rd25];
	add.s64 	%rd26, %rd20, %rd9;
	shl.b64 	%rd27, %rd26, 2;
	add.s64 	%rd7, %rd2, %rd27;
	ld.global.f32 	%f3, [%rd7];
	mul.rn.f32 	%f4, %f2, %f3;
	sub.rn.f32 	%f5, %f1, %f4;
	st.global.f32 	[%rd22], %f5;
	setp.eq.s64 	%p6, %rd4, %rd9;
	@%p6 bra 	LBB7_3;
	add.s64 	%rd29, %rd2, %rd21;
	ld.global.f32 	%f6, [%rd29];
	add.s64 	%rd31, %rd2, %rd24;
	ld.global.f32 	%f7, [%rd31];
	ld.global.f32 	%f8, [%rd7];
	mul.rn.f32 	%f9, %f7, %f8;
	sub.rn.f32 	%f10, %f6, %f9;
	st.global.f32 	[%rd29], %f10;
LBB7_3:
	ret;

}
	// .globl	set_zero
.visible .entry set_zero(
	.param .u64 set_zero_param_0,
	.param .u64 set_zero_param_1,
	.param .u64 set_zero_param_2,
	.param .u64 set_zero_param_3,
	.param .u32 set_zero_param_4,
	.param .u32 set_zero_param_5
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<19>;

	ld.param.u64 	%rd4, [set_zero_param_2];
	mov.u32 	%r1, %tid.x;
	cvt.s64.s32 	%rd5, %r1;
	ld.param.u64 	%rd6, [set_zero_param_3];
	ld.param.u32 	%rd7, [set_zero_param_4];
	mov.u32 	%r2, %ctaid.x;
	cvt.s64.s32 	%rd8, %r2;
	ld.param.u32 	%rd9, [set_zero_param_5];
	mul.lo.s64 	%rd10, %rd8, %rd7;
	add.s64 	%rd11, %rd10, %rd5;
	mov.u32 	%r3, %tid.y;
	cvt.s64.s32 	%rd12, %r3;
	mov.u32 	%r4, %ctaid.y;
	cvt.s64.s32 	%rd13, %r4;
	mul.lo.s64 	%rd14, %rd13, %rd9;
	add.s64 	%rd15, %rd14, %rd12;
	setp.ge.s64 	%p1, %rd11, %rd4;
	setp.ge.s64 	%p2, %rd15, %rd4;
	or.pred  	%p3, %p1, %p2;
	setp.eq.s64 	%p4, %rd11, %rd6;
	or.pred  	%p5, %p4, %p3;
	setp.ne.s64 	%p6, %rd15, %rd6;
	or.pred  	%p7, %p6, %p5;
	@%p7 bra 	LBB8_2;
	ld.param.u64 	%rd2, [set_zero_param_0];
	cvta.to.global.u64 	%rd3, %rd2;
	mul.lo.s64 	%rd16, %rd11, %rd4;
	add.s64 	%rd17, %rd16, %rd6;
	shl.b64 	%rd18, %rd17, 2;
	add.s64 	%rd1, %rd3, %rd18;
	mov.u32 	%r5, 0;
	st.global.u32 	[%rd1], %r5;
LBB8_2:
	ret;

}

